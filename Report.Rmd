---
title: "Time Series Analysis"
author: "Jan Dudzik"
date: "6/2/2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    keep_md: yes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# ARIMA model

##### As the first task was to analyze the non-seasonal time series, monthly mean of 3-month interest rate in Sweden will be used. Time series comes from the Eurostat database.

```{r, echo=FALSE}
library(forecast)
library(seastests)
library(ggplot2)
library(ggfortify)
source("functions04.R")
library(readr)
IR <- read_delim("IR.csv", ",", escape_double = FALSE, 
                    trim_ws = TRUE)
Sweden= ts(data=IR$Value, frequency = 12,             
           start=c(1993,1), end=c(2019,12)) 


```
```{r, echo=FALSE}

autoplot(Sweden, main = "3-month interest rate in Sweden - mean monthly value", xlab="Year", ylab="Interest rate", lty=1)

autoplot(diff(Sweden), main = "First difference of 3-month interest rate in Sweden - mean monthly value", xlab="Year", ylab="Interest rate", lty=1)

summary(wo(Sweden))

``` 

##### We can spot no seasonality by W-O test. Therefore we initiate ARIMA model analysis.

### 2.1 Integration level

```{r, echo=FALSE}
testdf(Sweden,4)
``` 

##### Despite stationarity reported by the Dickey-Fuller test in the original time series, we cannot rely on that test, because autocorrelation between residuals has been spotted by the Breusch-Godfrey test and that violates assumptions of Dickey-Fuller test.
```{r, echo=FALSE}
testdf(diff(Sweden),4)
``` 

##### Using the first differences of Sweden interest rates time series, with p-value = 0.8369650 we don't reject the null hypothesis of no autocorrelation between residuals and reject the null hypothesis of Dickey-Fuller test (p-value < 1percent) about non-stationarity of time series. Therefore we conclude, that the first differences are a stationary time series. We will confirm that using Kwiatkowski-Phillips-Schmidt-Shin test (KPSS)


```{r, echo=FALSE}
library(tseries)
kpss.test(diff(Sweden))
``` 

##### By not rejecting the null hypothesis of KPSS test with p-value = 0.1, we confirm stationarity of first differences
  
### 2.2 Parameters p and q identification

```{r, echo=FALSE}
ggAcf(diff(Sweden), lag.max = 30)
ggPacf(diff(Sweden), lag.max = 30)

``` 

##### Auto Correlation Function suggests Moving Average with q parameter = 7, and Partial Auto Correlation Function suggests p = 3. In consequence, we will analyze maximal model ARIMA(3,1,7) and respectively lower parameters.

```{r, echo=FALSE}
ar317 <- Arima(Sweden, order  = c(3,1,7))
ar316 <- Arima(Sweden, order  = c(3,1,6))
ar315 <- Arima(Sweden, order  = c(3,1,5))
ar314 <- Arima(Sweden, order  = c(3,1,4))
ar313 <- Arima(Sweden, order  = c(3,1,3))
ar312 <- Arima(Sweden, order  = c(3,1,2))
ar311 <- Arima(Sweden, order  = c(3,1,1))
ar310 <- Arima(Sweden, order  = c(3,1,0))

ar217 <- Arima(Sweden, order  = c(2,1,7))
ar216 <- Arima(Sweden, order  = c(2,1,6))
ar215 <- Arima(Sweden, order  = c(2,1,5))
ar214 <- Arima(Sweden, order  = c(2,1,4))
ar213 <- Arima(Sweden, order  = c(2,1,3))
ar212 <- Arima(Sweden, order  = c(2,1,2))
ar211 <- Arima(Sweden, order  = c(2,1,1))
ar210 <- Arima(Sweden, order  = c(2,1,0))

ar117 <- Arima(Sweden, order  = c(1,1,7))
ar116 <- Arima(Sweden, order  = c(1,1,6))
ar115 <- Arima(Sweden, order  = c(1,1,5))
ar114 <- Arima(Sweden, order  = c(1,1,4))
ar113 <- Arima(Sweden, order  = c(1,1,3))
ar112 <- Arima(Sweden, order  = c(1,1,2))
ar111 <- Arima(Sweden, order  = c(1,1,1))
ar110 <- Arima(Sweden, order  = c(1,1,0))

ar017 <- Arima(Sweden, order  = c(0,1,7))
ar016 <- Arima(Sweden, order  = c(0,1,6))
ar015 <- Arima(Sweden, order  = c(0,1,5))
ar014 <- Arima(Sweden, order  = c(0,1,4))
ar013 <- Arima(Sweden, order  = c(0,1,3))
ar012 <- Arima(Sweden, order  = c(0,1,2))
ar011 <- Arima(Sweden, order  = c(0,1,1))
ar010 <- Arima(Sweden, order  = c(0,1,0))
```

##### Now we need to find the best fitted ARIMA model, by the values of Akaike and Bayesian Information Criterion

```{r, echo=FALSE}
aic <- AIC(ar317,ar316,ar315,ar314,ar313,ar312,ar311,ar310, ar217,ar216,ar215,ar214,ar213,ar212,ar211,ar210,ar117,ar116,ar115,ar114,ar113,ar112,ar111,ar110,ar017,ar016,ar015,ar014,ar013,ar012,ar011,ar010 )
head(aic[order(aic$AIC),c(1,2)])
bic <- BIC(ar317,ar316,ar315,ar314,ar313,ar312,ar311,ar310, ar217,ar216,ar215,ar214,ar213,ar212,ar211,ar210,ar117,ar116,ar115,ar114,ar113,ar112,ar111,ar110,ar017,ar016,ar015,ar014,ar013,ar012,ar011,ar010 )
head(bic[order(bic$BIC),c(1,2)])
``` 

##### As the ar212 is the best ARIMA model by the Akaike criterion, and second-best with BIC, we can assume that's the best-suited model for this time series. Therefore we need to check if it's residuals can be classified as a White Noise

```{r, echo=FALSE}
ggAcf(ar212$residuals, lag.max = 30)
ggPacf(ar212$residuals, lag.max = 30)
Box.test(ar212$residuals, type = "Ljung-Box", lag = 25)
```

##### According to visual analysis and Ljung-Box test, there are no other lags and the model's residuals can be classified as a White Noise (p-value = 0.8375). 

### 2.3 Prediction of the last three periods
##### As the ARIMA model contains trend, and no seasonality was detected, we can use Holt method in order to predict future values. the prediction will be examined by comparison with ARIMA forecast from forecast library.
```{r, echo=FALSE}

train_data = ts(data=IR$Value[1:321], frequency = 12,             
                         start=c(1993,1), end=c(2019,9)) 
test_data = ts(data=IR$Value[321:324], frequency = 12,             
                start=c(2019,9), end=c(2019,12)) 

library(forecast)
par(mfrow = c(1,2))
holt<-holt(train_data, h=3)
plot(holt, lty=1, xlim=c(as.Date(2017), as.Date(2020)), main = "")
lines(test_data, col="black", lty=1)

ar212_in_sample <- Arima(train_data, order  = c(2,1,2))
forecast = (forecast(ar212_in_sample, h = 3))
plot(forecast, xlim=c(as.Date(2017), as.Date(2020)), main = "")
lines(test_data, col="black", lty=1)

```

##### The plot on the left presents prediction of last three period's values generated by Holt method (represented by blue dots) and the real values (black line) of time series, while the plot on the right compares ARIMA forecast with real values. Predictions on both models looks close to the real values, so we will examine their precision by MAE, MSE, MAPE and SMAPE statistics

```{r, include=FALSE}
library(DescTools)
stats <- c("MAE", "MSE","MAPE","SMAPE")
Holt_stats <- forecast_stats <- c(round(MAE(holt$mean, test_data), 5),round(MSE(holt$mean, test_data), 5),round(MAPE(holt$mean, test_data), 5),round(SMAPE(holt$mean, test_data), 5))
forecast_stats <- c(round(MAE(forecast$mean, test_data), 5),round(MSE(forecast$mean, test_data), 5),round(MAPE(forecast$mean, test_data), 5),round(SMAPE(forecast$mean, test_data), 5))
stats <- rbind(stats, Holt_stats, forecast_stats)
stats
```
```{r, echo = FALSE}

stats
```

##### Prediction provided by Holt's method generates smaller Mean Absolute Error, Mean Squared Error and Mean Absolute Percentage Error than ARIMA model, but is a worse prediction according to Symmetric Mean Absolute Percentage Error Statistics

# SARIMA model

### 3.1. Data description

##### Provided data is a monthly amount of airplane passengers in the United Kingdom between years 1993 and 2019. Data originate from Eurostat database and used variable is Passengers carried, so it reports the sum of people on board of an flights, that originated or destinated in the United Kingdom in the reported month 

```{r, echo = FALSE}
library(readxl)
seasonal <- read_excel("pasazÌ‡erowieUK.xls")
passengers = ts(data=seasonal$Liczba, frequency = 12,             
                      start=c(1993,1), end=c(2019,11)) 

autoplot(passengers, main = "United Kingdom airplane passengers 1993-2019", ts.colour = 'red',xlab="Year", ylab="Number of passengers", lty=3)

```

##### The plot above represents the values of the analysed time series. Clearly there exists some seasonality, and definitely there is no stationarity in this time series. Therefore, twelth differences are used to continue analysis. 

```{r, echo = FALSE}
d.passengers <- diff(passengers, 12)
tsdisplay(d.passengers)
```

##### After using 12th difference, seasonality is hardly spotted, but as the time series is still non - stationary, I use first differences

```{r, echo = FALSE}
d.d.passengers <- diff(d.passengers, 11)
tsdisplay(d.d.passengers)
testdf(d.d.passengers, adf_order = 3)
kpss.test(d.d.passengers)
```

##### After using another differencing, time series seems to be stationary, so with the usage of the Dickey-Fuller test and KPSS test, stationarity is proven. With Breusch-Godfrey test assuring about no autocorrelation of residuals, stationarity is proven by p-value <1 percent of the Augmented Dickey-Fuller test. What is more, the Kwiatkowski-Phillips-Schmidt-Shin test, with p-value = 0.1, does not reject the null hypothesis of stationarity of time series. 

```{r, echo=FALSE}

ggAcf(d.d.passengers, lag.max = 36)


```

##### By analysis of 12th lags of AutoCorrelation Function, we can spot promptly descending correlation, so the P parameter in seasonal AR model is recognized as 1. By analysis of firs lags, we can spot three significant lags, then p parameter equal to 3 in non-seasonal Auto-Regressive model will be considered

```{r, echo=FALSE}

ggPacf(d.d.passengers, lag.max = 36)

```

##### As a result of Partial AutoCorrelation Function analysis, seasonal Moving Average parameter Q is recognized as 1, and the non-seasonal MA parameter is pointed out as 2, 3, or 8. 


```{r, echo = FALSE}

sarima_312_111 <- arima(passengers, order = c(3,1,2), seasonal = list(order = c(1,1,1), period = 12))
sarima_313_111 <- arima(passengers, order = c(3,1,3), seasonal = list(order = c(1,1,1), period = 12))
sarima_318_111 <- arima(passengers, order = c(3,1,8), seasonal = list(order = c(1,1,1), period = 12))

```
##### Three SARIMA models are considered: SARIMA (312),(111),12; SARIMA (313),(111),12 and SARIMA (318),(111),12. 
##### Built-in Auto Arima function with seasonality will be used as an another option to consider
```{r, include=FALSE}
library(tseries)
```
```{r, echo = FALSE}
auto.arima(passengers, stepwise = TRUE, approximation = FALSE, seasonal = TRUE)
```
##### As the auto.arima function decided, the best fitted model is SARIMA (202),(111),12, so another SARIMA model will be calculated

```{r, echo= FALSE}

sarima_202_111 <- arima(passengers, order = c(2,0,2), seasonal = list(order = c(1,1,1), period = 12))

```

##### All models residuals randomness will be tested by Ljung-Box test

```{r, echo=FALSE}
Box.test(sarima_312_111$residuals, type = "Ljung-Box")
Box.test(sarima_313_111$residuals, type = "Ljung-Box")
Box.test(sarima_318_111$residuals, type = "Ljung-Box")
Box.test(sarima_202_111$residuals, type = "Ljung-Box")
```

##### As Ljung-Box test rejected the null hypothesis of autocorrelations across model's residuals for all SARIMA models (every p-value greater than 0.05), the best model will be chosen by AIC and BIC criterions

```{r, echo=FALSE}
AIC(sarima_312_111, sarima_313_111, sarima_318_111)
AIC(sarima_202_111)
BIC(sarima_312_111, sarima_313_111, sarima_318_111)
BIC(sarima_202_111)
```

##### The conclusion of Akaike Information Cryterion and Bayesian Information Cryterion is that the best suited model is SARIMA (318),(111),12

### SARIMA prediction 
##### To predict time series continuation, described by SARIMA models Holt-Winters method will be used. We distinguish two types of this method: additive and multiplicative. Both of the approaches will be examined.
```{r, echo=FALSE}
train_data = ts(data=seasonal$Liczba[1:191], frequency = 12,             
                              start=c(1993,1), end=c(2018,11)) 
pass.hw.add<-hw(train_data, h=20,seasonal="additive")
pass.hw.mult<-hw(train_data, h=20,seasonal="multiplicative")

plot(passengers)
lines(fitted(pass.hw.add), col="orange", lty=2)
lines(fitted(pass.hw.mult), col="green", lty=2)
```

##### The plot above shows how well fitted are two applied types of Holt-Winters methods. In next step, prediction based on those models will be generated and examined by MAE, MSE, MAPE and SMAPE statictics. In addition, predictions of SARIMA model will also be examined.
```{r, echo=FALSE}
sarima_318_111 <- arima(train_data, order = c(3,1,8), seasonal = list(order = c(1,1,1), period = 12))

forecast.sarima = (forecast(sarima_318_111, h = 12))

holt.test1 = forecast(HoltWinters(train_data, alpha = 0.175, beta = 0.0143, gamma = 0.3223, seasonal = "additive"), h = 12)
holt.test2 = forecast(HoltWinters(train_data, alpha = 0.1679, beta = 0.0109, gamma = 0.3856, seasonal = "multiplicative"), h = 12)

test_data = ts(data=seasonal$Liczba[192:203], frequency = 12,             
               start=c(2018,12), end=c(2019,11))

stats <- c("MAE", "MSE","MAPE","SMAPE")
sarima_stats <- forecast_stats <- c(round(MAE(forecast.sarima$mean, test_data), 5),round(MSE(forecast.sarima$mean, test_data), 5),round(MAPE(forecast.sarima$mean, test_data), 5), round(SMAPE(forecast.sarima$mean, test_data), 5))
holt_add_stats <- c(round(MAE(holt.test1$mean, test_data), 5), round(MSE(holt.test1$mean, test_data), 5),round(MAPE(holt.test1$mean, test_data), 5),round(SMAPE(holt.test1$mean, test_data), 5))
holt_mult_stats <- c(round(MAE(holt.test2$mean, test_data), 5), round(MSE(holt.test2$mean, test_data), 5),round(MAPE(holt.test2$mean, test_data), 5),round(SMAPE(holt.test2$mean, test_data), 5))

stats <- rbind(stats, sarima_stats, holt_add_stats, holt_mult_stats)
stats

```

##### According to MAE, MSE, MAPE and SMAPE statistics, Holt-Winters multiplied method, gaining the lowest score in every statictics, was detected as the best prediction approach. Predicted values of different approaches are shown on the plot below

```{r, echo=FALSE}

plot(passengers, xlim=c(as.Date(2017), as.Date(2020)))
lines(holt.test2$mean, col = "green", lty = 2)
lines(holt.test1$mean, col = "yellow", lty = 2)
lines(forecast.sarima$mean, col = "red", lty = 2)
legend("topright",
        legend=c("original values", "Holt-Winters additive", "Holt-Winters multiplicative", "SARIMA forecast"),
        col=c("black", "green", "yellow", "red"), lty=c(1,2,2,2))
```


##### As a final conclusion, two datasets were examined, time series were recognised as ARIMA (2,1,2) and SARIMA (3,1,8),(1,1,1),12 models and their forecasts were provided by ARIMA forecast, Holt, SARIMA forecast, additive Holt-Winters and multiplicative Holt-Winters methods.