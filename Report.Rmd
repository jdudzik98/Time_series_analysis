---
title: "Time Series Analysis"
author: "Jan Dudzik"
date: "6/2/2020"
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# ARIMA model

### 1. Data description
Provided data shows the Price Index of goods and services in Poland between the years 2000 and 2020. The basic prices value reflects December of 1999 and is set as a 100, so basically, every record is a mean value of selected month’s prices in goods and services divided by the mean value of this sector’s prices in December 1999 and multiplied by 100.


```{r}
library(readxl)
prices_data <- read_excel("Szereg_niesezon.xls")

prices= ts(data=prices_data$TOWARYUSLUGI, frequency = 12,             
             start=c(2000,1), end=c(2020,2)) 


```

```{r, echo=FALSE}
library(ggplot2)
library(ggfortify)

autoplot(prices, main = "Level of prices (December 1999 = 100)", ts.colour = 'red',xlab="Year", ylab="Prices level", lty=3)
autoplot(diff(prices), main = "First differences in prices level (December 1999 = 100)", ts.colour = 'blue',xlab="Year", ylab="Difference of price level", lty=3)

```

According to initial plots, however, this particular time series seems to be non-stationary, the first differences could be. Therefore I will examine that by the Dickey-Fuller test.

### 2. Integration level
```{r include=FALSE}
library(forecast)
source("functions04.R")
testdf(prices, 4)

``` 
```{r}

testdf(prices, 4)

``` 

Analysis of original time series has proven, that according to the Dickey-Fuller Test, it is not stationary (p-value > 10 percent), that proves visual conclusion. What is more, according to the Breusch-Godfrey test with no lags, the null hypothesis about no autocorrelation of residuals is rejected.
```{r, echo = FALSE}
testdf(diff(prices), 4)

``` 

Statistical tests of first differences of selected time series prove, that according to Dickey-Fuller test, we cannot reject the null hypothesis of non-stationarity (p-value < 1 percent), and by p-value = 0.7993760 of Breusch-Godfrey test, there is no autocorrelation between residuals in this model

```{r include=FALSE}
library(forecast)

``` 
```{r}

ggAcf(diff(prices), lag.max = 30)

``` 

Although using the AutoCorrelation Function, we can spot a seasonality in this time series. This conjecture is proven by W-O test (Webel Ollech)

```{r}

library(seastests)
summary(wo(prices))
``` 

# Second ARIMA model

As the first task was to analyze the non-seasonal time series, we need to change data. Therefore I will use a monthly mean of 3-month interest rate in Sweden. Time series comes from the Eurostat database.


```{r}

library(readr)
IR <- read_delim("IR.csv", ",", escape_double = FALSE, 
                    trim_ws = TRUE)
Sweden= ts(data=IR$Value, frequency = 12,             
           start=c(1993,1), end=c(2019,12)) 


```
```{r, echo=FALSE}

autoplot(Sweden, main = "3-month interest rate in Sweden - mean monthly value", xlab="Year", ylab="Interest rate", lty=1)

autoplot(diff(Sweden), main = "3-month interest rate in Sweden - mean monthly value", xlab="Year", ylab="Interest rate", lty=1)

summary(wo(Sweden))

``` 

We can spot no seasonality by W-O test. Therefore we initiate ARIMA model analysis.

### 2.1 Integration level

```{r}
testdf(Sweden,4)
``` 

Despite stationarity reported by the Dickey-Fuller test in the original time series, we cannot rely on that test, because autocorrelation between residuals has been spotted by the Breusch-Godfrey test and that violates assumptions of Dickey-Fuller test.
```{r}
testdf(diff(Sweden),4)
``` 

Using the first differences of Sweden interest rates time series, with p-value = 0.8369650 we don't reject the null hypothesis of no autocorrelation between residuals and reject the null hypothesis of Dickey-Fuller test (p-value < 1percent) about non-stationarity of time series. Therefore we conclude, that the first differences are a stationary time series. We will confirm that using Kwiatkowski-Phillips-Schmidt-Shin test (KPSS)


```{r}
library(tseries)
kpss.test(diff(Sweden))
``` 

By not rejecting the null hypothesis of KPSS test with p-value = 0.1, we confirm stationarity of first differences
  
### 2.2 Parameters p and q identification

```{r}
ggAcf(diff(Sweden), lag.max = 30)
ggPacf(diff(Sweden), lag.max = 30)

``` 

Auto Correlation Function suggests Moving Average with q parameter = 7, and Partial Auto Correlation Function suggests p = 3. In consequence, we will analyze maximal model ARIMA(3,1,7) and respectively lower parameters.

```{r}
ar317 <- Arima(Sweden, order  = c(3,1,7))
ar316 <- Arima(Sweden, order  = c(3,1,6))
ar315 <- Arima(Sweden, order  = c(3,1,5))
ar314 <- Arima(Sweden, order  = c(3,1,4))
ar313 <- Arima(Sweden, order  = c(3,1,3))
ar312 <- Arima(Sweden, order  = c(3,1,2))
ar311 <- Arima(Sweden, order  = c(3,1,1))
ar310 <- Arima(Sweden, order  = c(3,1,0))

ar217 <- Arima(Sweden, order  = c(2,1,7))
ar216 <- Arima(Sweden, order  = c(2,1,6))
ar215 <- Arima(Sweden, order  = c(2,1,5))
ar214 <- Arima(Sweden, order  = c(2,1,4))
ar213 <- Arima(Sweden, order  = c(2,1,3))
ar212 <- Arima(Sweden, order  = c(2,1,2))
ar211 <- Arima(Sweden, order  = c(2,1,1))
ar210 <- Arima(Sweden, order  = c(2,1,0))

ar117 <- Arima(Sweden, order  = c(1,1,7))
ar116 <- Arima(Sweden, order  = c(1,1,6))
ar115 <- Arima(Sweden, order  = c(1,1,5))
ar114 <- Arima(Sweden, order  = c(1,1,4))
ar113 <- Arima(Sweden, order  = c(1,1,3))
ar112 <- Arima(Sweden, order  = c(1,1,2))
ar111 <- Arima(Sweden, order  = c(1,1,1))
ar110 <- Arima(Sweden, order  = c(1,1,0))

ar017 <- Arima(Sweden, order  = c(0,1,7))
ar016 <- Arima(Sweden, order  = c(0,1,6))
ar015 <- Arima(Sweden, order  = c(0,1,5))
ar014 <- Arima(Sweden, order  = c(0,1,4))
ar013 <- Arima(Sweden, order  = c(0,1,3))
ar012 <- Arima(Sweden, order  = c(0,1,2))
ar011 <- Arima(Sweden, order  = c(0,1,1))
ar010 <- Arima(Sweden, order  = c(0,1,0))
```

Now we need to find the best fitted ARIMA model, by the values of Akaike and Bayesiand Information Criterion

```{r}
aic <- AIC(ar317,ar316,ar315,ar314,ar313,ar312,ar311,ar310, ar217,ar216,ar215,ar214,ar213,ar212,ar211,ar210,ar117,ar116,ar115,ar114,ar113,ar112,ar111,ar110,ar017,ar016,ar015,ar014,ar013,ar012,ar011,ar010 )
head(aic[order(aic$AIC),c(1,2)])
bic <- BIC(ar317,ar316,ar315,ar314,ar313,ar312,ar311,ar310, ar217,ar216,ar215,ar214,ar213,ar212,ar211,ar210,ar117,ar116,ar115,ar114,ar113,ar112,ar111,ar110,ar017,ar016,ar015,ar014,ar013,ar012,ar011,ar010 )
head(bic[order(bic$BIC),c(1,2)])
``` 

As the ar212 is the best ARIMA model by the Akaike cryterion, and second best with BIC, we can assume that's the best suited model for this time series. Therefore we need to check if it's residuals can be classified as a White Noise

```{r}
ggAcf(ar212$residuals, lag.max = 30)
ggPacf(ar212$residuals, lag.max = 30)
Box.test(ar212$residuals, type = "Ljung-Box", lag = 25)
```

